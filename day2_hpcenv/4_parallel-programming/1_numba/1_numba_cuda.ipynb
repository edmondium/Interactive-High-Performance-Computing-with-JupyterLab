{
 "cells": [
  {
   "attachments": {
    "75fbcf54-0e30-4011-a97f-1124259fa39e.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:75fbcf54-0e30-4011-a97f-1124259fa39e.png)\n",
    "\n",
    "Numba is an **open source JIT compiler** that translates a subset of Python and NumPy code into fast machine code.  \n",
    "Numba also works great with Jupyter notebooks **for interactive computing**, and with distributed execution frameworks, like Dask and Spark.  \n",
    "https://numba.pydata.org\n",
    "\n",
    "- [x] **Accelerate Python Functions**\n",
    "  - Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library.\n",
    "- [x] **Built for Scientific Computing**\n",
    "  - Numba is designed to be used with NumPy arrays and functions.\n",
    "  - Numba generates specialized code for different array data types and layouts to optimize performance.\n",
    "  - Special decorators can create universal functions that broadcast over NumPy arrays just like NumPy functions do.\n",
    "- [x] **Parallelize Your Algorithms**\n",
    "  - Numba offers a range of options for parallelizing your code for CPUs and GPUs, often with only minor code changes.\n",
    "  \n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whscBGwEDcEZ"
   },
   "source": [
    "example from: https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb  \n",
    "blog on: https://thedatafrog.com/en/articles/boost-python-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03jodrvKLJAP"
   },
   "source": [
    "## A gentle introduction to ufuncs\n",
    "\n",
    "**Numpy universal functions** or `ufuncs` are functions that operate on a numpy array in an element-by-element fashion.  \n",
    "For example, when we take the square of a numpy array, a `ufunc` computes the square of each element before returning the resulting array:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hw8e1l8iLpH1",
    "outputId": "c6b6de20-2d4c-4458-c367-cec379065d29"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hw8e1l8iLpH1",
    "outputId": "c6b6de20-2d4c-4458-c367-cec379065d29"
   },
   "outputs": [],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2hF7-vzLys6"
   },
   "source": [
    "Most **math functions** (if not all) are available as `ufuncs` in numpy.  \n",
    "For example, to exponentiate all elements in a numpy array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "1n2hkLgbLx86",
    "outputId": "07e7b9c6-c222-4733-85da-445ed8396feb"
   },
   "outputs": [],
   "source": [
    "np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwkXetz9MInV"
   },
   "source": [
    "Most `ufuncs` are **implemented in compiled C code**, so they are already quite fast, and much faster than plain python.  \n",
    "For example, let's consider a large array, and let's compute the logarithm of each element, **both in plain python and in numpy**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "v_EU7lwrNBUP",
    "outputId": "1bed24c6-1679-4368-a632-65f6e5b041f1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "x = np.arange(int(1e6))\n",
    "%timeit np.sqrt(x)\n",
    "%timeit [math.sqrt(xx) for xx in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xwQGTwVNXEr"
   },
   "source": [
    "We see that the numpy `ufunc` is more than **100 times faster**.  \n",
    "Still, in the `ufunc`, the calculation is **not parallelized**: the square root is computed sequentially on the CPU for each element in the array. \n",
    "\n",
    "The **GPU**, contrary to the CPU, is able to perform a large number of **operations simultaneously**.    \n",
    "\n",
    "**In the following,**  \n",
    "we will see how to create and compile `ufuncs` for the GPU to perform the calculation on many elements at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk2lkx_LEJhN"
   },
   "source": [
    "## A very simple example : parallel square root calculation on the GPU \n",
    "\n",
    "Our first `ufunc` **for the GPU** will again compute the square root for a large number of points. \n",
    "\n",
    "We start by building a **sample of points ranging from 0 to 10 millions**. GPUs are more efficient with numbers that are encoded on a small number of bits. And often, a very high precision is not needed. So we create a sample of `float32` numbers (the default being `float64`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CZsUw4REvNW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npoints = int(1e7)\n",
    "a = np.arange(npoints, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_rQxpVQP6Qo"
   },
   "source": [
    "**With numba**, we can create `ufuncs` **compiled for the CPU** using the vectorize decorator.  \n",
    "Let's start by doing this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "xT64SF4DQR1V",
    "outputId": "2e1a47c0-e218-42e5-cb5c-f52baf9c125c"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import vectorize \n",
    "\n",
    "@vectorize\n",
    "def cpu_sqrt(x):\n",
    "  return math.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "xT64SF4DQR1V",
    "outputId": "2e1a47c0-e218-42e5-cb5c-f52baf9c125c"
   },
   "outputs": [],
   "source": [
    "cpu_sqrt(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhGMg8bSEwLw"
   },
   "source": [
    "Creating a `ufunc` **for the GPU** is almost as straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuaHYsSTnZml"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import vectorize\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_pnAX86QoQGq",
    "outputId": "e4d5e754-2b8a-4da4-b826-6ea7fb31f626"
   },
   "outputs": [],
   "source": [
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IygNUitPxNi"
   },
   "source": [
    "It is important to note that, contrary to the CPU case, **the input and return types** of the function have to be specified, when compiling for the GPU.  \n",
    "In the string: \n",
    "\n",
    "```\n",
    "'float32(float32)'\n",
    "```\n",
    "\n",
    "The first `float32` corresponds to the return type, and the second one to the input type.  \n",
    "You can think of it as: the returned value is a function of the input value.  \n",
    "**Also, please note** that these types need to be adapted to your data.  \n",
    "In the case above, we have created an array of `float32` values, so we are fine. \n",
    "\n",
    "**Exercise:** Edit the type definition in the vectorize decorator to read `float64`, and see what happens when you call the function. Then change it back to `float32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npFHbvbNGZ0k"
   },
   "source": [
    "Now let's see how much we gained. For this, we use the timeit magic command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DNvfK-TeGBI3",
    "outputId": "861fd310-4780-4571-ad46-8456a11ed235"
   },
   "outputs": [],
   "source": [
    "%timeit gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRQYI0JjGrNB"
   },
   "source": [
    "The GPU managed to compute the sqrt for 10 million points in 14 ms.  \n",
    "Now let's see what we get with numpy, which is compiled for the CPU, and with our CPU `ufunc`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NPsSrGcyF5-a",
    "outputId": "e9c05364-61b1-4911-a1b8-b21a88adda09"
   },
   "outputs": [],
   "source": [
    "%timeit np.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "r5cgsw2_Q4ZL",
    "outputId": "ccee7b57-23be-4de8-f9eb-1d24e7d9ce4c"
   },
   "outputs": [],
   "source": [
    "%timeit cpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO1gsEtnF360"
   },
   "source": [
    "**Wait! We do not gain anything and the CPU version is actually twice faster!** \n",
    "\n",
    "There is a simple reason for this. When running on the GPU, the following happens under the hood: \n",
    "\n",
    "* the input data (the array `a`) is transferred to the GPU memory;\n",
    "* the calculation of the square root is done in parallel on the GPU for all elements of `a`;\n",
    "* the resulting array is sent back to the host system. \n",
    "\n",
    "If the calculation is too simple, there is no use shipping our data to the GPU for fast parallel processing, if we are to wait so long for the data transfers to complete. In other words, most of the time is spent in the data transfers, and the GPU is basically useless.\n",
    "\n",
    "Let's see what happens with a more involved calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrze4fYGM2zu"
   },
   "source": [
    "## From cartesian to polar coordinates on the GPU\n",
    "\n",
    "Let's build an array of 1000 points in 2D, described by the cartesian coordinates x and y.  \n",
    "We choose to draw the points according to a 2D Gaussian distribution, introducing some correlation between the x and y components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFMuCJTzNYcj"
   },
   "outputs": [],
   "source": [
    "points = np.random.multivariate_normal([0,0], [[1.,0.9], [0.9,1.]], 1000).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "jvAFfCXbvyI3",
    "outputId": "dae8cc84-863f-40cd-b6ca-39b01b5bcdd4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(points[:,0], points[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7zHafAWNTsn"
   },
   "source": [
    "It's often useful to convert cartesian coordinates into the polar coordinates `R` and `theta`,  \n",
    "where r is the distance of the point to origin, and where theta is the angle between the x axis and the direction of the point. \n",
    "\n",
    "![](https://raw.githubusercontent.com/cbernet/maldives/master/numba/cart_polar.png)\n",
    "\n",
    "`R` is easily obtained as the quadratic sum of x and y: \n",
    "\n",
    "$$R = \\sqrt{x^2 + y^2}$$\n",
    "\n",
    "But to get theta, one has to use the `arctan2` function.  \n",
    "This function is available in numpy, and we can use it to easily get the results  \n",
    "(please note that the y coordinate has to be provided as the first argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "_QuMci1UOFMX",
    "outputId": "b9985859-d6d6-4a23-d78f-35e4c1fa3b6d"
   },
   "outputs": [],
   "source": [
    "theta = np.arctan2(points[:,1], points[:,0]) \n",
    "_ = plt.hist(theta, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrBoDk1fPWVE"
   },
   "source": [
    "Because of the correlation between x and y, we see two peaks at $\\pi/4$ and $-3\\pi/4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrBoDk1fPWVE"
   },
   "source": [
    "Now let's try and perform the same calculation on the GPU.  \n",
    "This time, we have two input values, and we define the function signature accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49E87JbarOOn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "@vectorize(['float32(float32, float32)'],target='cuda')\n",
    "def gpu_arctan2(y, x): \n",
    "    theta = math.atan2(y,x)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma4OnucmRw4r"
   },
   "source": [
    "As before with `np.arctan2`, we need to slice our `points` array to provide first the array of `y` coordinates, and then the array of `x` coordinates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "Lw8BMFAr8tS2",
    "outputId": "963c530b-8c02-46c7-dc8a-20afb7af39b4"
   },
   "outputs": [],
   "source": [
    "theta = gpu_arctan2(points[:,1], points[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR_hD0ndRNo3"
   },
   "source": [
    "**Hmm this doesn't work.**\n",
    "As the exception says, it is because `points[:,1]` contain values that are not contiguous in memory, and same for `points[:,2]`.  \n",
    "Memory representation of `points` = `xyxyxyxyxyxyxyxyxy...`\n",
    "\n",
    "So we do as instructed and convert these slices into contiguous arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctVE6lhG9bBP"
   },
   "outputs": [],
   "source": [
    "x = np.ascontiguousarray(points[:,0])\n",
    "y = np.ascontiguousarray(points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXCtUNAg9o74"
   },
   "outputs": [],
   "source": [
    "theta = gpu_arctan2(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "LQeyFTil9vJB",
    "outputId": "bb5d9ea8-315e-41ae-d936-ea499a7f9dfc"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(theta, bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziyMhQf9SoQV"
   },
   "source": [
    "And now it works! \n",
    "\n",
    "**As a general rule, one should remember that  \n",
    "CUDA operates on data buffers that are contiguous in memory, like a C array, or a numpy array before any slicing.**\n",
    "\n",
    "Now let's be a bit more ambitious, and compute theta for 10 million points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb6O__sjr1tt"
   },
   "outputs": [],
   "source": [
    "points = np.random.multivariate_normal([0,0], [[1.,0.9], [0.9,1.]], int(1e7)).astype(np.float32)\n",
    "x = np.ascontiguousarray(points[:,0])\n",
    "y = np.ascontiguousarray(points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "fdvX8t7bA4An",
    "outputId": "361a018c-b51c-4370-8489-835598f922ab"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(gpu_arctan2(y, x), bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_4uMeLBS1Kg"
   },
   "source": [
    "And finally, let's quantify how much time we gain by running on the GPU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dnQ7DGgvsGtf",
    "outputId": "5c7376cf-0743-4cc4-fb3b-a53c9c512993"
   },
   "outputs": [],
   "source": [
    "%timeit gpu_arctan2(y, x) # compiled for the GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "33XHREIXAWNg",
    "outputId": "15632d00-280e-4c17-cfdd-fdab985c2733"
   },
   "outputs": [],
   "source": [
    "%timeit np.arctan2(y, x) # compiled for the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "s8fz5tC8HGgY",
    "outputId": "9a7fa172-163c-4a01-b3b1-43dd4240dd8d"
   },
   "outputs": [],
   "source": [
    "%timeit [math.atan2(point[1], point[0]) for point in points] # plain python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snh32iOSTIQL"
   },
   "source": [
    "**Nice!** this time we gain more than a factor 10 on the GPU with respect to the numpy version, which is compiled for the CPU.  \n",
    "And this comes on top of a factor 300 gain with respect to plain python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m86np5r3VMnS"
   },
   "source": [
    "**Remember: To use the GPU efficiently, you need to give it enough data to process, and complicated tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RFdj6vnSgRz"
   },
   "source": [
    "## Generalized ufuncs on the GPU\n",
    "\n",
    "In regular `ufuncs`, the calculation is done on each element of the input array, and returns a scalar. \n",
    "\n",
    "In generalized ufuncs (`gufuncs`), however, the calculation can **deal with a sub-array of the input array**,   \n",
    "and return an array of different dimensions.\n",
    "\n",
    "Let's have a look at a few examples, things are going to become much clearer. \n",
    "\n",
    "### Generalized ufuncs : from cartesian to polar coordinates\n",
    "\n",
    "Remember how we computed the polar angle for our 2D points above?  \n",
    "It would have been nice to get both `rho` and `theta` from this calculation, to get a real conversion from cartesian to polar coordinates.  \n",
    "This kind of thing is not possible with regular `ufuncs` but with `gufuncs`, it's easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34HCaUDhAiAq"
   },
   "outputs": [],
   "source": [
    "from numba import guvectorize\n",
    "\n",
    "@guvectorize(['(float32[:], float32[:])'],\n",
    "             '(i)->(i)',                \n",
    "             target='cuda')\n",
    "def gpu_polar(vec, out):\n",
    "    x = vec[0]\n",
    "    y = vec[1]\n",
    "    out[0] = math.sqrt(x**2 + y**2)\n",
    "    out[1] = math.atan2(y,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24pYBeIpXmrc"
   },
   "source": [
    "There are two imporant differences between `guvectorize` and `vectorize`. \n",
    "\n",
    "For `guvectorize`: \n",
    "\n",
    "* **one needs to provide the signature of the array operation**.  \n",
    "The signature of the array operation should **not to confused with the signature of the compiled function** that is provided as a first argument.  \n",
    "In the example above, `(i)->(i)` means that a 1D array is taken in input, and that a 1D array with the same size is provided in the output. The 1D array corresponds to the last dimension or innermost dimension of the input array. For example, our points array is of shape `(10000000,2)` so the last dimension is of size 2.\n",
    "* **the result is taken in input and modified in place**. In the code above, the resulting polar coordinates R and theta are stored in the `out` array while `vec`, the input array, contains the cartesian coordinates `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B9nynrzarZW"
   },
   "source": [
    "Let's do the conversion between cartesian and polar coordinates: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmKqnMVBUynE"
   },
   "outputs": [],
   "source": [
    "polar_coords = gpu_polar(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "y0GtKNeNWhnp",
    "outputId": "30f65394-a144-4c1c-853d-2004b212c8f0"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(polar_coords[:,0], bins=200)\n",
    "_ = plt.xlabel('R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "N_lnYsK-XBsx",
    "outputId": "f0ba4ea0-2a3a-4e50-a9e0-f44a3d6311df"
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(polar_coords[:,1], bins=200)\n",
    "_ = plt.xlabel('theta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXSIQjDbccSV"
   },
   "source": [
    "### Generalized ufunc: Average \n",
    "\n",
    "To understand better how gufuncs work, let's make one that computes the average of the values on each line of a 2D array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZRhx6PrU2Es"
   },
   "outputs": [],
   "source": [
    "@guvectorize(['(float32[:], float32[:])'],\n",
    "             '(n)->()',                \n",
    "             target='cuda')\n",
    "def gpu_average(array, out):\n",
    "    acc = 0\n",
    "    for val in array: \n",
    "        acc += val\n",
    "    out[0] = acc/len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCd7mfkifgbK"
   },
   "source": [
    "To test our gufunc, we create a 2D array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "Ee1MM1B2fbHV",
    "outputId": "403e74eb-f555-41a2-c242-d49943e521be"
   },
   "outputs": [],
   "source": [
    "a = np.arange(100).reshape(20, 5).astype(np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "p8TTRs5Pfpy0",
    "outputId": "42dc1912-6c87-47af-fe41-e7043eadf508"
   },
   "outputs": [],
   "source": [
    "gpu_average(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0_4_Io3QnNp"
   },
   "source": [
    "## Device functions \n",
    "\n",
    "So far, we have run a single function, either a ufunc or a gufunc, on the GPU, but we are not forced to put all of our code in a single function.\n",
    "\n",
    "Indeed, it is also  possible to compile helper functions for the GPU.  \n",
    "These functions, called **device functions**, can then be used on the GPU to make the code cleaner and more modular. \n",
    "\n",
    "As an example, let's take again the gufunc defined just above, that computes the average of the values of each line of a 2D array. \n",
    "\n",
    "We define a device function to add using the `numba.cuda.jit` decorator, to sum up the elements of a 1D array. Then, we modify the gpu_average gufunc to make use of the `add` device function. And finally, we create another gufunc to sum up the elements of on each line of a 2D array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9XdnHRBfunN"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def add(array): \n",
    "  acc = 0\n",
    "  for val in array: \n",
    "    acc += val\n",
    "  return acc\n",
    "\n",
    "@guvectorize(['(float32[:], float32[:])'],\n",
    "             '(n)->()',                \n",
    "             target='cuda')\n",
    "def gpu_average_2(array, out):\n",
    "    out[0] = add(array)/len(array)\n",
    "\n",
    "@guvectorize(['(float32[:], float32[:])'],\n",
    "             '(n)->()',                \n",
    "             target='cuda')\n",
    "def gpu_sum(array, out):\n",
    "    out[0] = add(array)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "cNzS_PeoSVBZ",
    "outputId": "ea7701cc-0d40-4fe7-9fec-9dcf5c6083f5"
   },
   "outputs": [],
   "source": [
    "gpu_average_2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "eTrkfYBPSjXT",
    "outputId": "09832783-426e-4e10-ca74-9dbbc57b409c"
   },
   "outputs": [],
   "source": [
    "gpu_sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuGYnrVkTbx4"
   },
   "source": [
    "The device function allows us to avoid code duplication. \n",
    "\n",
    "Obviously, it is a bit artifial to use a device function in such an easy case. But when implementing complex algorithms, these functions can prove very useful. Indeed, just like on the CPU, the general principles of programming apply: functions should be simple and to the point, and code duplication should be avoided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U0yngpWU1Sg"
   },
   "source": [
    "## Memory management with device arrays\n",
    "\n",
    "As we have seen in the first `ufunc` example given in this article (parallel square root calculation), the GPU does not always provide a gain in performance. \n",
    "\n",
    "Indeed, before using the raw computing power of the GPU, we need to ship the data to the device. And afterwards, we need to get the results back. \n",
    "\n",
    "A good way to improve performance is to **minimize data transfers between the host system and the GPU**, and this can be done with device arrays. \n",
    "\n",
    "To illustrate this, we will use an example provided by nvidia in its DLI course. Let's assume we want to implement a neural network for image processing from scratch. A hidden layer in the network might have to do the following: \n",
    "\n",
    "* normalize greyscale values in the image\n",
    "* weigh them\n",
    "* apply an activation function \n",
    "\n",
    "Each of these three tasks can be done in parallel on the GPU. \n",
    "\n",
    "But first, let's see how to do that on the CPU with plain numpy. \n",
    "For simplicity, we will generate the greyscale values and the weights randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFl7DB9wTZu5"
   },
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "\n",
    "# random values between 0. and 255.\n",
    "greyscales = np.floor(np.random.uniform(0, 256, n).astype(np.float32))\n",
    "# random weights following a Gaussian distribution\n",
    "# centred on 0.5 and with width 0.1\n",
    "weights = np.random.normal(.5, .1, n).astype(np.float32)\n",
    "\n",
    "def normalize(grayscales):\n",
    "    return grayscales / 255\n",
    "\n",
    "def weigh(values, weights):\n",
    "    return values * weights\n",
    "        \n",
    "def activate(values):\n",
    "    return ( np.exp(values) - np.exp(-values) ) / \\\n",
    "            ( np.exp(values) + np.exp(-values) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "EirTdzaSY7BH",
    "outputId": "7c442d8d-1193-4a51-a0c9-31a746a07c7b"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "normalized = np.float32(normalize(greyscales))\n",
    "weighted = weigh(normalized, weights)\n",
    "activated = activate(weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6b4CSoHZ_dO"
   },
   "source": [
    "Now, we implement a parallel version of this algorithm for the GPU, as we have seen above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d6Y2mgBZqyl"
   },
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32)'],target='cuda')\n",
    "def gpu_normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "@vectorize(['float32(float32, float32)'],target='cuda')\n",
    "def gpu_weigh(x, w):\n",
    "  return x * w\n",
    "\n",
    "@vectorize(['float32(float32)'],target='cuda')\n",
    "def gpu_activate(x): \n",
    "  return ( math.exp(x) - math.exp(-x) ) / ( math.exp(x) + math.exp(-x) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq0Hui5Nar4M"
   },
   "source": [
    "Please note that the code in these `ufuncs` operates on scalar values, so we replaced the numpy `ufuncs` like np.exp by their math equivalent (the division by 255 and the multiplication between the values and the weights were also numpy ufuncs, though hidden a bit). \n",
    "\n",
    "And we check the performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "y1ge-bW_ajh5",
    "outputId": "557d2c1d-3774-4efd-d6e8-327f7896159d"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "normalized = gpu_normalize2(greyscales)\n",
    "weighted = gpu_weigh(normalized, weights)\n",
    "activated = gpu_activate(weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoOELkQgbUnq"
   },
   "source": [
    "That's already quite nice, we gained more than a factor of two! \n",
    "\n",
    "But we realize that we spend time transferring data back and forth between the host and the GPU for nothing: \n",
    "\n",
    "1. transfer `greyscales` to the GPU\n",
    "1. transfer `normalized` to the host, and then back to the GPU, together with `weights`\n",
    "1. transfer `weighted` to the host, and then back to the GPU\n",
    "1. transfer `activated` to the host\n",
    "\n",
    "Actually, we only need to: \n",
    "\n",
    "1. transfer `greyscales` and `weights` to the GPU\n",
    "1. retrieve `activated`\n",
    "\n",
    "So let's do that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GCZHkmXhbKb8",
    "outputId": "6db9ee9c-69cf-42dc-d24a-c1c74886f773"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# create intermediate arrays on the GPU\n",
    "normalized_gpu = cuda.device_array(shape=(n,), \n",
    "                               dtype=np.float32)\n",
    "weighted_gpu = cuda.device_array(shape=(n,), \n",
    "                             dtype=np.float32)\n",
    "\n",
    "# note that output device arrays are provided as arguments \n",
    "gpu_normalize(greyscales, out=normalized_gpu)\n",
    "gpu_weigh(normalized_gpu, weights, out=weighted_gpu)\n",
    "activated = gpu_activate(weighted_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdPH1RdJeV41"
   },
   "source": [
    "We gain a factor of two by eliminating unnecessary data transfers! \n",
    "\n",
    "Another important thing to know is that we can also take full control on the transfers to and from the GPU like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1n43lAIdWGl"
   },
   "outputs": [],
   "source": [
    "# transfer inputs to the gpu\n",
    "greyscales_gpu = cuda.to_device(greyscales)\n",
    "weights_gpu = cuda.to_device(weights)\n",
    "\n",
    "# create intermediate arrays and output array on the GPU\n",
    "normalized_gpu = cuda.device_array(shape=(n,), \n",
    "                               dtype=np.float32)\n",
    "weighted_gpu = cuda.device_array(shape=(n,), \n",
    "                             dtype=np.float32)\n",
    "activated_gpu = cuda.device_array(shape=(n,), \n",
    "                             dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCTA-6f8fNu_"
   },
   "source": [
    "Now that everything we need is on the GPU, we do the calculation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4D3tldT0fLh_",
    "outputId": "d2d93787-c9db-4171-ec92-3cac01fe816d"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gpu_normalize(greyscales_gpu, out=normalized_gpu)\n",
    "gpu_weigh(normalized_gpu, weights_gpu, out=weighted_gpu)\n",
    "gpu_activate(weighted_gpu, out=activated_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kywGvXpIfcGe"
   },
   "source": [
    "You might be thinking that this factor 5 gain is artificial, because we did not include the necessary transfer times for the input and output data. That's right! Still, this illustrates how you can take full control of your data transfers, which might prove useful on more complex processing workflows. \n",
    "\n",
    "For instance, if we wanted to re-use any of the device arrays defined above, we could do it now, as they are still residing on the GPU as I'm writing this!\n",
    "\n",
    "Finally, here is how to retrieve the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Jzs4oDbQfY8p",
    "outputId": "a30cceb4-eed2-4fae-ff34-1cab97186275"
   },
   "outputs": [],
   "source": [
    "activated = activated_gpu.copy_to_host()\n",
    "activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von numba_cuda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
